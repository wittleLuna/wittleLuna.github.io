<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>GAT on wittleLuna&#39;s blog</title>
    <link>http://localhost:1313/en/tags/gat/</link>
    <description>Recent content from wittleLuna&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    
    <managingEditor>syaz23277@gmail.com (wittleLuna)</managingEditor>
    <webMaster>syaz23277@gmail.com (wittleLuna)</webMaster>
    
    <copyright>All articles on this blog are licensed under the BY-NC-SA license agreement unless otherwise stated. Please indicate the source when reprinting!</copyright>
    
    <lastBuildDate>Tue, 09 Sep 2025 14:36:58 +0800</lastBuildDate>
    
    
    <atom:link href="http://localhost:1313/en/tags/gat/index.xml" rel="self" type="application/rss&#43;xml" />
    

    
    

    <item>
      <title>[Deep Learning]Graph Attention Network, Graph Neural Network</title>
      <link>http://localhost:1313/en/post/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Tue, 09 Sep 2025 00:00:00 &#43;0000</pubDate>
      <author>syaz23277@gmail.com (wittleLuna)</author>
      <guid>http://localhost:1313/en/post/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>
        <![CDATA[<h1>[Deep Learning]Graph Attention Network, Graph Neural Network</h1><p>Author: wittleLuna(syaz23277@gmail.com)</p>
        
          <h2 id="1-信息聚合方式的区别">
<a class="header-anchor" href="#1-%e4%bf%a1%e6%81%af%e8%81%9a%e5%90%88%e6%96%b9%e5%bc%8f%e7%9a%84%e5%8c%ba%e5%88%ab"></a>
1. 信息聚合方式的区别
</h2><h3 id="-传统-gnn如-gcn">
<a class="header-anchor" href="#-%e4%bc%a0%e7%bb%9f-gnn%e5%a6%82-gcn"></a>
🔹 传统 GNN（如 GCN）
</h3><ul>
<li>采用 <strong>固定的归一化权重</strong> 来聚合邻居节点特征。</li>
<li>公式示例（GCN）：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">h_i&#39; = \sigma\left( \sum_{j \in \mathcal{N}(i)} \frac{1}{\sqrt{d_i d_j}} W h_j \right)
</span></span></code></pre></div><p>这里权重 $\frac{1}{\sqrt{d_i d_j}}$ 仅依赖于节点度数，是预定义的，不随数据学习。
➡ 聚合方式是 <strong>静态的</strong>，缺少自适应性。</p>
<h3 id="-图注意力网络gat--gatv2">
<a class="header-anchor" href="#-%e5%9b%be%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bd%91%e7%bb%9cgat--gatv2"></a>
🔹 图注意力网络（GAT / GATv2）
</h3><ul>
<li>采用 <strong>自注意力机制</strong> 来为邻居分配权重。</li>
<li>公式：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">h_i&#39; = \sigma\left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} W h_j \right), \quad
</span></span><span class="line"><span class="cl">\alpha_{ij} = \text{softmax}_j \big( a^T \, \text{LeakyReLU}(W[h_i \, || \, h_j]) \big)
</span></span></code></pre></div><p>其中 alpha_{ij} 是 <strong>可学习的权重</strong>，依赖于节点特征本身。
➡ 聚合方式是 <strong>动态的</strong>，模型能根据任务自动决定哪些邻居更重要。</p>
        
        <hr><p>Published on 2025-09-09 at <a href='http://localhost:1313/'>wittleLuna's blog</a>, last modified on 2025-09-09</p>]]>
      </description>
      
        <category>Deep Learning</category>
      
    </item>
    
  </channel>
</rss>
