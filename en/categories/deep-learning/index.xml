<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning on wittleLuna&#39;s blog</title>
    <link>https://wittleLuna.github.io/en/categories/deep-learning/</link>
    <description>Recent content from wittleLuna&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    
    <managingEditor>syaz23277@gmail.com (wittleLuna)</managingEditor>
    <webMaster>syaz23277@gmail.com (wittleLuna)</webMaster>
    
    <copyright>All articles on this blog are licensed under the BY-NC-SA license agreement unless otherwise stated. Please indicate the source when reprinting!</copyright>
    
    <lastBuildDate>Tue, 09 Sep 2025 14:36:58 +0800</lastBuildDate>
    
    
    <atom:link href="https://wittleLuna.github.io/en/categories/deep-learning/index.xml" rel="self" type="application/rss&#43;xml" />
    

    
    

    <item>
      <title>[Deep Learning]Residual Connection</title>
      <link>https://wittleLuna.github.io/en/post/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/</link>
      <pubDate>Tue, 09 Sep 2025 00:00:00 &#43;0000</pubDate>
      <author>syaz23277@gmail.com (wittleLuna)</author>
      <guid>https://wittleLuna.github.io/en/post/%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5/</guid>
      <description>
        <![CDATA[<h1>[Deep Learning]Residual Connection</h1><p>Author: wittleLuna(syaz23277@gmail.com)</p>
        
          <h2 id="-什么是残差连接">
<a class="header-anchor" href="#-%e4%bb%80%e4%b9%88%e6%98%af%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5"></a>
📖 什么是残差连接
</h2><ul>
<li><strong>残差连接</strong>是深度学习里的一种 <strong>网络结构设计技巧</strong>，最早在 <strong>ResNet（残差网络）</strong> 中提出。</li>
<li>它的核心思想是：
👉 <strong>让输入绕过若干层神经网络，直接加到输出上。</strong></li>
</ul>
<p>公式：</p>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">y = F(x) + x
</span></span></code></pre></div><p>其中：</p>
<ul>
<li>x = 输入</li>
<li>F(x) = 若干层神经网络的变换</li>
<li>y = 输出（包含了原始输入 + 新学到的变化）</li>
</ul>
<h2 id="-为什么要用残差连接">
<a class="header-anchor" href="#-%e4%b8%ba%e4%bb%80%e4%b9%88%e8%a6%81%e7%94%a8%e6%ae%8b%e5%b7%ae%e8%bf%9e%e6%8e%a5"></a>
📌 为什么要用残差连接
</h2><ol>
<li>
<p><strong>缓解梯度消失/爆炸</strong></p>
<ul>
<li>在非常深的神经网络里，梯度可能传不下去，导致训练困难。</li>
<li>残差连接给梯度提供了一条“捷径”，更容易训练。</li>
</ul>
</li>
<li>
<p><strong>避免退化问题</strong></p>
<ul>
<li>网络越深，不一定越好，有时反而性能下降。</li>
<li>残差连接让深层网络至少能“模仿浅层网络”，避免性能变差。</li>
</ul>
</li>
<li>
<p><strong>更易学习</strong></p>
<ul>
<li>网络不需要学完整的映射 H(x)，只需学“残差” F(x) = H(x) - x，任务更简单。</li>
</ul>
</li>
</ol>
<h2 id="-直观理解">
<a class="header-anchor" href="#-%e7%9b%b4%e8%a7%82%e7%90%86%e8%a7%a3"></a>
🎯 直观理解
</h2><ul>
<li>没有残差：网络必须学会 <strong>从 0 到目标函数</strong> 的完整映射。</li>
<li>有了残差：网络只需在“已有的输入基础上，做一些修正”。</li>
</ul>
<p>👉 就像写作文：不是从零开始写，而是拿一篇草稿（输入）做修改（残差），效率更高。</p>
<h2 id="-一句话总结">
<a class="header-anchor" href="#-%e4%b8%80%e5%8f%a5%e8%af%9d%e6%80%bb%e7%bb%93"></a>
✅ 一句话总结
</h2><p><strong>残差连接就是在网络里给输入开一条“捷径”，把输入直接加到输出上，帮助训练更深的神经网络。</strong></p>
        
        <hr><p>Published on 2025-09-09 at <a href='https://wittleLuna.github.io/'>wittleLuna's blog</a>, last modified on 2025-09-09</p>]]>
      </description>
      
        <category>Deep Learning</category>
      
    </item>
    
    

    <item>
      <title>[Deep Learning]Graph Attention Network, Graph Neural Network</title>
      <link>https://wittleLuna.github.io/en/post/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</link>
      <pubDate>Tue, 09 Sep 2025 00:00:00 &#43;0000</pubDate>
      <author>syaz23277@gmail.com (wittleLuna)</author>
      <guid>https://wittleLuna.github.io/en/post/%E5%9B%BE%E6%B3%A8%E6%84%8F%E5%8A%9B%E7%BD%91%E7%BB%9C-%E5%9B%BE%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</guid>
      <description>
        <![CDATA[<h1>[Deep Learning]Graph Attention Network, Graph Neural Network</h1><p>Author: wittleLuna(syaz23277@gmail.com)</p>
        
          <h2 id="1-信息聚合方式的区别">
<a class="header-anchor" href="#1-%e4%bf%a1%e6%81%af%e8%81%9a%e5%90%88%e6%96%b9%e5%bc%8f%e7%9a%84%e5%8c%ba%e5%88%ab"></a>
1. 信息聚合方式的区别
</h2><h3 id="-传统-gnn如-gcn">
<a class="header-anchor" href="#-%e4%bc%a0%e7%bb%9f-gnn%e5%a6%82-gcn"></a>
🔹 传统 GNN（如 GCN）
</h3><ul>
<li>采用 <strong>固定的归一化权重</strong> 来聚合邻居节点特征。</li>
<li>公式示例（GCN）：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">h_i&#39; = \sigma\left( \sum_{j \in \mathcal{N}(i)} \frac{1}{\sqrt{d_i d_j}} W h_j \right)
</span></span></code></pre></div><p>这里权重 $\frac{1}{\sqrt{d_i d_j}}$ 仅依赖于节点度数，是预定义的，不随数据学习。
➡ 聚合方式是 <strong>静态的</strong>，缺少自适应性。</p>
<h3 id="-图注意力网络gat--gatv2">
<a class="header-anchor" href="#-%e5%9b%be%e6%b3%a8%e6%84%8f%e5%8a%9b%e7%bd%91%e7%bb%9cgat--gatv2"></a>
🔹 图注意力网络（GAT / GATv2）
</h3><ul>
<li>采用 <strong>自注意力机制</strong> 来为邻居分配权重。</li>
<li>公式：</li>
</ul>
<div class="highlight"><pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">h_i&#39; = \sigma\left( \sum_{j \in \mathcal{N}(i)} \alpha_{ij} W h_j \right), \quad
</span></span><span class="line"><span class="cl">\alpha_{ij} = \text{softmax}_j \big( a^T \, \text{LeakyReLU}(W[h_i \, || \, h_j]) \big)
</span></span></code></pre></div><p>其中 alpha_{ij} 是 <strong>可学习的权重</strong>，依赖于节点特征本身。
➡ 聚合方式是 <strong>动态的</strong>，模型能根据任务自动决定哪些邻居更重要。</p>
        
        <hr><p>Published on 2025-09-09 at <a href='https://wittleLuna.github.io/'>wittleLuna's blog</a>, last modified on 2025-09-09</p>]]>
      </description>
      
        <category>Deep Learning</category>
      
    </item>
    
  </channel>
</rss>
