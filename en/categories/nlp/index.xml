<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>NLP on wittleLuna&#39;s blog</title>
    <link>http://localhost:1313/en/categories/nlp/</link>
    <description>Recent content from wittleLuna&#39;s blog</description>
    <generator>Hugo</generator>
    <language>en</language>
    
    <managingEditor>syaz23277@gmail.com (wittleLuna)</managingEditor>
    <webMaster>syaz23277@gmail.com (wittleLuna)</webMaster>
    
    <copyright>All articles on this blog are licensed under the BY-NC-SA license agreement unless otherwise stated. Please indicate the source when reprinting!</copyright>
    
    <lastBuildDate>Tue, 25 Feb 2025 14:36:58 +0800</lastBuildDate>
    
    
    <atom:link href="http://localhost:1313/en/categories/nlp/index.xml" rel="self" type="application/rss&#43;xml" />
    

    
    

    <item>
      <title>The Introduction of Natural Language Processing (NLP)</title>
      <link>http://localhost:1313/en/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</link>
      <pubDate>Tue, 25 Feb 2025 00:00:00 &#43;0000</pubDate>
      <author>syaz23277@gmail.com (wittleLuna)</author>
      <guid>http://localhost:1313/en/post/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/</guid>
      <description>
        <![CDATA[<h1>The Introduction of Natural Language Processing (NLP)</h1><p>Author: wittleLuna(syaz23277@gmail.com)</p>
        
          <h2 id="背景知识">
<a class="header-anchor" href="#%e8%83%8c%e6%99%af%e7%9f%a5%e8%af%86"></a>
背景知识
</h2><p>自然语言处理(natural language processing, NLP)</p>
<h3 id="nlp发展历程">
<a class="header-anchor" href="#nlp%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b"></a>
NLP发展历程
</h3><p><strong>阶段一：图灵测试</strong>
该测试的流程是，一名测试者写下自己的问题，随后将问题以纯文本的形式（如计算机屏幕和键盘）发送给另一个房间中的一个人与一台机器。测试者根据他们的回答来判断哪一个是真人，哪一个是机器。所有参与测试的人或机器都会被分开。这个测试旨在探究机器能否模拟出与人类相似或无法区分的智能 [1]。
现在的图灵测试测试时长通常为5分钟，如果电脑能回答由人类测试者提出的一系列问题，且其超过30%的回答让测试者误认为是人类所答，则电脑通过测试。</p>
<p><strong>阶段二：基于规则的方法</strong>
自然语言处理的最早阶段主要采用基于规则的方法，通过人工定义语法和规则来解析和生成文本。这些方法的局限性在于难以涵盖语言的复杂性和多样性，因为规则需要人为设计且难以适应不同的语境。</p>
<p><strong>阶段三：统计学习方法</strong>
随着统计学习方法的兴起，自然语言处理进入了统计学习阶段。该阶段的代表性方法包括隐马尔可夫模型（Hidden Markov Models，HMM）和最大熵模型（Maximum Entropy Models）。这些方法通过从大量语料中学习统计规律来解决语言处理问题，提高了模型的泛化能力。</p>
<p><strong>阶段四：深度学习与神经网络</strong>
深度学习的广泛应用推动了自然语言处理领域的进一步发展。循环神经网络（Recurrent Neural Networks，RNN）和长短时记忆网络（Long Short-Term Memory，LSTM）等模型在序列标注、机器翻译等任务上取得了显著的成果。随后，注意力机制和Transformer模型的提出进一步提升了自然语言处理的性能，例如BERT、GPT等模型。</p>
<h3 id="ai发展历程">
<a class="header-anchor" href="#ai%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b"></a>
AI发展历程
</h3><p>第一阶段-AI兴起：人工智能的诞生（1941- 1956）</p>
<p>第二阶段-AI早期成功：AI黄金发展时代（1956-1974）</p>
<p>第三阶段-AI第一次寒冬：神经网络遇冷，研究经费减少（1974~1980）</p>
<p>第四阶段-AI复兴：第二次AI黄金发展时代，专家系统流行并商用（1980~1987）</p>
<p>第五阶段-AI第二次寒冬：专家系统溃败，研究经费大减（1987~1993）</p>
<p>第六阶段-AI崛起：深度学习理论和工程突破（1993至今）</p>
<h3 id="语言模型">
<a class="header-anchor" href="#%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b"></a>
语言模型
</h3><p>用于捕捉自然语言中词汇，短语和句子的概率分布的统计模型。根据给定的上下文，预测接下来出现的单词。</p>
<h3 id="语料库">
<a class="header-anchor" href="#%e8%af%ad%e6%96%99%e5%ba%93"></a>
语料库
</h3><p>大量自然语言文本的集合</p>
<ul>
<li>通用领域语料库</li>
<li>特定领域语料库</li>
</ul>
<h3 id="数据预处理">
<a class="header-anchor" href="#%e6%95%b0%e6%8d%ae%e9%a2%84%e5%a4%84%e7%90%86"></a>
数据预处理
</h3><p><strong>数据采集</strong>：获取文本数据，可以来自各种来源，如网页、社交媒体、新闻文章、文本文件等。数据采集可能需要网络爬虫或API调用。</p>
<p><strong>文本清洗</strong>：清除不需要的字符、符号、HTML标签等。这通常涉及使用正则表达式和文本处理库来进行清洗。清洗后的文本更易于分析和处理。</p>
<p><strong>分词</strong>：将文本分割成单词或标记。分词是将文本数据转化为机器可理解的基本单位，有助于构建词汇表和分析文本结构。</p>
<p><strong>停用词去除</strong>：停用词是常见的无实际信息的词语，如“the”、“and”等。通常需要将它们从文本中去除，以减小词汇表的大小。</p>
<p><strong>词干提取和词形还原</strong>：这有助于将单词还原为其基本形式，以减少词汇多样性。例如，将“running”还原为“run”。</p>
<p><strong>特征提取</strong>：将文本转化为数值特征，例如词袋模型、TF-IDF权重等。这是将文本数据转化为可以用于机器学习模型的数值表示的重要步骤。</p>
<p><strong>数据标记和标签</strong>：对文本数据进行标记和分类，以便用于监督学习任务，如文本分类或命名实体识别。</p>
<h3 id="统计语言模型的发展历程">
<a class="header-anchor" href="#%e7%bb%9f%e8%ae%a1%e8%af%ad%e8%a8%80%e6%a8%a1%e5%9e%8b%e7%9a%84%e5%8f%91%e5%b1%95%e5%8e%86%e7%a8%8b"></a>
统计语言模型的发展历程
</h3><p>出现很早，但由于网络结构和数据量的限制，早期并没有实现突破性应用。存在不少缺点：过拟合，无法处理文本间长距离依赖性等。</p>
<ul>
<li>
<p>1948 N-Gram模型：基于前N-1个词来预测第N个词</p>
</li>
<li>
<p>1954 Bag-of-word模型: 将一个句子或文档表示为单词的集合，不考虑单词在文本中的顺序。不能捕捉语义语法信息。</p>
</li>
<li>
<p>1986 分布式表示法：将单词或短语表示为数值向量的方法。具有较多的语法和句意信息。解决了Bag-of-Word模型和独热编码中的词汇鸿沟问题。</p>
</li>
<li>
<p>2003 神经概率语言模型: 通过神经网络对语言进行建模</p>
</li>
<li>
<p>2013 WordtoVec: 通过训练神经网络模型来学习词汇的分布式表示，简单又高效。两种架构：CBOW模型和Skip-Gram模型。可以捕捉到单词之间的相似性和语义语法信息。</p>
</li>
<li>
<p>2018以后 基于Transformer的预训练语言模型：通过更大的语料库和更复杂的神经网络体系结构来进行语法语义的学习</p>
        
        <hr><p>Published on 2025-02-25 at <a href='http://localhost:1313/'>wittleLuna's blog</a>, last modified on 2025-02-25</p>]]>
      </description>
      
        <category>NLP</category>
      
    </item>
    
  </channel>
</rss>
